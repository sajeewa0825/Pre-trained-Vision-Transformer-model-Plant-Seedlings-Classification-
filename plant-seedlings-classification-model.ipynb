{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12014283,"sourceType":"datasetVersion","datasetId":7558476}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers timm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom transformers import ViTForImageClassification, ViTFeatureExtractor\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport shutil","metadata":{"colab_type":"code","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dir = \"/kaggle/input/plant-seedlings-classification/train\"\nlabels = os.listdir(train_dir)\n\n# Create a List of (image_path, label)\ndata = []\nfor label in labels:\n    image_paths = os.listdir(os.path.join(train_dir, label))\n    for img_name in image_paths:\n        img_path = os.path.join(train_dir, label, img_name)\n        data.append((img_path, label))\n\ndf = pd.DataFrame(data, columns=[\"image_path\", \"label\"])\n\n\n# Dataset Summary\nprint(\"Total images:\", len(df))\nprint(\"Number of classes:\", df['label'].nunique())\nprint(\"Classes:\", df['label'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.sample(n=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check missing value\nprint(\"Missing image paths:\", df['image_path'].isnull().sum())\nprint(\"Missing labels:\", df['label'].isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for completely duplicated rows (all columns)\nduplicates = df[df.duplicated()]\nprint(f\"Total completely duplicated rows: {len(duplicates)}\")\n\n# Check for duplicate image paths only\ndup_images = df[df.duplicated(subset=['image_path'])]\nprint(f\"Duplicate image paths: {len(dup_images)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visual Inspection (1 sample per class)\n\nplt.figure(figsize=(15, 8))\nunique_labels = df['label'].unique()\n\nfor i, label in enumerate(unique_labels):\n    sample_path = df[df['label'] == label].sample(1)['image_path'].values[0]\n    image = Image.open(sample_path)\n    \n    plt.subplot(3, 5, i+1)\n    plt.imshow(image)\n    plt.title(label)\n    plt.axis('off')\n\nplt.tight_layout()\nplt.suptitle(\"Sample Image per Class\", y=1.02, fontsize=16)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Map Labels to IDs - Encode labels\nlabel2id = {label: idx for idx, label in enumerate(sorted(df['label'].unique()))}\nid2label = {v: k for k, v in label2id.items()}\ndf[\"label_id\"] = df[\"label\"].map(label2id)\n\ntrain_df, val_df = train_test_split(df, test_size=0.1, stratify=df[\"label_id\"], random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Missing image paths:\", df['image_path'].isnull().sum())\nprint(\"Missing labels:\", df['label'].isnull().sum())\nprint(\"Missing label_ids:\", df['label_id'].isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_counts = df['label'].value_counts()\nprint(class_counts)\n\n# Visualize\nplt.figure(figsize=(12,5))\nsns.barplot(x=class_counts.index, y=class_counts.values)\nplt.title(\"Original Class Distribution\")\nplt.xticks(rotation=90)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"augment_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a new augmented directory\naug_dir = \"./augmented_train\"\nos.makedirs(aug_dir, exist_ok=True)\n\ntarget_count = class_counts.max()\n\nbalanced_records = []\n\nfor label in class_counts.index:\n    class_dir = os.path.join(aug_dir, label)\n    os.makedirs(class_dir, exist_ok=True)\n\n    class_df = df[df['label'] == label]\n    n_existing = len(class_df)\n    samples = class_df.sample(n=target_count, replace=True, random_state=42)\n\n    for i, row in samples.iterrows():\n        img = Image.open(row['image_path']).convert(\"RGB\")\n\n        # Augment if itâ€™s synthetic sample\n        if n_existing <= target_count and i >= n_existing:\n            img = augment_transform(img)\n\n        # Save image\n        filename = f\"{label}_{i}_{random.randint(1000, 9999)}.png\"\n        save_path = os.path.join(class_dir, filename)\n        img.save(save_path)\n\n        balanced_records.append({\n            \"image_path\": save_path,\n            \"label\": label,\n            \"label_id\": row[\"label_id\"]\n        })\n\n# Create new balanced dataframe\ndf = pd.DataFrame(balanced_records)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_counts = df['label'].value_counts()\nprint(class_counts)\n\nplt.figure(figsize=(12, 6))\nsns.countplot(x='label', data=df)\nplt.xticks(rotation=90)\nplt.title(\"Class Distribution\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PlantSeedlingsDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe.reset_index(drop=True)\n        self.transform = transform\n        \n    # Tell PyTorch how many items in dataset\n    def __len__(self):\n        return len(self.dataframe)\n\n    #Load and return one image and its label as a tensor\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n        label = row[\"label_id\"]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n\n\n# Define Train Transform\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), #Converts image to a PyTorch tensor\n    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std) #Standardize pixel values\n])\n\n# Define Validation Transform\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), # Converts image to a PyTorch tensor\n    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std) #Standardize pixel values\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = PlantSeedlingsDataset(train_df, transform=train_transform)\nval_dataset = PlantSeedlingsDataset(val_df, transform=val_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.nn import CrossEntropyLoss\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = ViTForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224\",\n    num_labels=len(label2id),\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True\n)\nmodel = model.to(device)\n\n# Count samples in each class\nclass_counts = df['label_id'].value_counts().sort_index()\nclass_weights = 1.0 / class_counts\nclass_weights = class_weights / class_weights.sum()\n\n# Convert to tensor and move to GPU if available\nweights_tensor = torch.tensor(class_weights.values, dtype=torch.float).to(device)\n\n# Create loss function with class weights\ncriterion = CrossEntropyLoss(weight=weights_tensor)\n\n# criterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=2e-5)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, loader, optimizer, criterion):\n    model.train()\n    running_loss = 0\n    correct = 0\n    total = 0\n\n    # Loop through each batch of images and labels\n    for images, labels in tqdm(loader):\n        images, labels = images.to(device), labels.to(device)\n\n        # Forward pass: Get model predictions (logits)\n        outputs = model(images).logits\n        # Compute loss between predictions and true labels\n        loss = criterion(outputs, labels)\n\n         # Zero out gradients from previous step\n        optimizer.zero_grad()\n        # Backward pass: compute gradients\n        loss.backward() # \n        # Update model weights based on gradients\n        optimizer.step()\n\n        running_loss += loss.item()\n        # Get predicted class indices (the one with highest logit score)\n        _, preds = outputs.max(1)\n        correct += preds.eq(labels).sum().item()\n        total += labels.size(0)\n\n    accuracy = 100. * correct / total\n    avg_loss = running_loss / len(loader)\n    return avg_loss, accuracy\n\ndef evaluate(model, loader, criterion):\n    model.eval()\n    running_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n\n            outputs = model(images).logits\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item()\n            _, preds = outputs.max(1)\n            correct += preds.eq(labels).sum().item()\n            total += labels.size(0)\n\n    accuracy = 100. * correct / total\n    avg_loss = running_loss / len(loader)\n    return avg_loss, accuracy\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS = 30\n\nfor epoch in range(EPOCHS):\n    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n    val_loss, val_acc = evaluate(model, val_loader, criterion)\n\n    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n    print(f\"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Make sure model is in evaluation mode\nmodel.eval()\n\n# Image transform (same as training)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n])\n\ndef predict_image(image_path):\n    image = Image.open(image_path).convert('RGB')\n    input_tensor = transform(image).unsqueeze(0).to(device)  # add batch dimension\n    with torch.no_grad():\n        outputs = model(pixel_values=input_tensor)\n        logits = outputs.logits\n        predicted_class_id = logits.argmax(dim=1).item()\n    return predicted_class_id, image\n\n# Show N predictions\ndef show_predictions(df, n=12):\n    plt.figure(figsize=(16, 8))\n    for i in range(n):\n        row = df.iloc[i]\n        pred_id, image = predict_image(row[\"image_path\"])\n        actual = row[\"label\"]\n        predicted = id2label[pred_id]\n\n        plt.subplot(2, n//2, i+1)\n        plt.imshow(image)\n        plt.title(f\"Actual: {actual}\\nPredicted: {predicted}\", fontsize=10)\n        plt.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\n# Example usage\nshow_predictions(val_df, n=12)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_path = \"./vit-plant-seedlings\"\n\n# Save model and processor\nmodel.save_pretrained(save_path)\nfeature_extractor.save_pretrained(save_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from huggingface_hub import HfApi\n\n# api = HfApi(token=os.getenv(\"\"))\n# api.upload_folder(\n#     folder_path=\"./vit-plant-seedlings\",\n#     repo_id=\"sajeewa/vit-base-patch16-224\",\n#     repo_type=\"model\",\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}